{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPolnQO2QFnPbvu0h2QQkdp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNBqayvpVhT7","outputId":"a365e35f-262b-42db-f46b-551ca15afee9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","#from tensorflow.keras import datasets, layers, models, regularizers, optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","# 시간정보를 활용하여 폴더 생성\n","import datetime \n","\n","#3채널 525*700 픽셀의 500개 이미지\n","IMG_CHANNELS = 3\n","IMG_ROWS = 525\n","IMG_COLS = 700\n","IMG_SIZE = (IMG_ROWS, IMG_COLS)\n","#########################################################\n","window = 48\n","data = np.load('/content/drive/MyDrive/dataset/augmented_Spectogram/train_' + str(window)+'.npz')\n","X_train = data['array1']\n","Y_train = data['array2']\n","\n","data = np.load('/content/drive/MyDrive/dataset/augmented_Spectogram/test_' + str(window)+'.npz')\n","argumented_X_test = data['array1']\n","argumented_Y_test = data['array2']\n","\n","#print(X_train.shape)\n","val_size = int(X_train.shape[0]/5)\n","#print(val_size)\n","########################################################\n","k =  1 # k 는 1에서 5\n","argumented_X_val = X_train[val_size*(k-1):val_size*k, :, :, :]\n","argumented_Y_val = Y_train[val_size*(k-1):val_size*k]\n","\n","argumented_X_train = np.concatenate((X_train[:val_size*(k-1), :, :, :] , X_train[val_size*k:, :, :, :] ),axis=0)\n","argumented_Y_train = np.concatenate((Y_train[:val_size*(k-1)] , Y_train[val_size*k:]),axis=0)\n","\n","# 상수\n","BATCH_SIZE =  100\n","EPOCHS = 100\n","NUM_CLASSES = 5\n","VERBOSE = 1\n","OPTIM = tf.keras.optimizers.RMSprop()\n","\n","INPUT_SHAPE = (428, 81, 3)\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, Flatten, Reshape, LSTM\n","\n","model = Sequential()\n","model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape= INPUT_SHAPE))\n","model.add(Conv1D(32, kernel_size=3, activation='relu'))\n","\n","model.add(Reshape((428*77, 32)))\n","model.add(LSTM(16, return_sequences=True))\n","model.add(Flatten())\n","model.add(Dense(5, activation='softmax'))\n","#model.summary()\n","OPTIM = tf.keras.optimizers.RMSprop()\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 텐서보드 사용\n","callbacks = [\n","    # 텐서보드 로그를 './logs' 디렉터리에 작성\n","    tf.keras.callbacks.TensorBoard(log_dir = './logs')\n","]\n","\n","# 학습데이터의 log를 저장할 폴더 생성 (지정)\n","log_dir = \"logs/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","# 텐서보드 콜백 정의 하기\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","history = model.fit(argumented_X_train, argumented_Y_train,\n","                    batch_size = BATCH_SIZE, epochs = EPOCHS,\n","                    verbose = VERBOSE, validation_data = (argumented_X_val, argumented_Y_val),\n","                    callbacks = [tensorboard_callback])\n","\n","score = model.evaluate(argumented_X_test, argumented_Y_test, batch_size = BATCH_SIZE)\n","print(\"\\nTest score:\", score[0])\n","print(\"Test accuracy:\", score[1])\n","\n","    # 모델 저장\n","model_title = \"/content/drive/MyDrive/model/coughdetection/lstm_\" + str(window) +\"_\" + str(k) + \".json\"\n","weights_title = '/content/drive/MyDrive/model/coughdetection/lstm_weights_' + str(window) +\"_\" + str(k) + \".h5\"\n","\n","model_json = model.to_json()\n","with open(model_title, 'w') as json_file:\n","  json_file.write(model_json)\n","model.save_weights(weights_title)\n"]}]}